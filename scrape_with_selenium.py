#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ Selenium çš„ Minecraft Wiki ç‰ˆæœ¬å†å²çˆ¬è™«
ç»•è¿‡åçˆ¬è™«é™åˆ¶
"""

import re
import json

from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import time

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


@dataclass
class VersionRecord:
    """ç‰ˆæœ¬è®°å½•æ•°æ®ç±»"""
    date: str
    time: str
    editor: str
    file_size: str
    change_bytes: str
    description: str
    wiki_url: str
    edit_url: str


class SeleniumScraper:
    """ä½¿ç”¨ Selenium çš„çˆ¬è™«"""
    
    def __init__(self):
        self.base_url = "https://minecraft.wiki"
        self.history_url = "https://minecraft.wiki/w/Java_Edition_protocol/Entity_metadata?action=history&limit=500&offset="
        self.driver = None
    
    def setup_driver(self):
        """è®¾ç½® Chrome é©±åŠ¨"""
        if not SELENIUM_AVAILABLE:
            print("âŒ Selenium æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install selenium")
            print("âŒ è¿˜éœ€è¦ä¸‹è½½ ChromeDriver: https://chromedriver.chromium.org/")
            return False
        
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— ç•Œé¢æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            return True
        except Exception as e:
            print(f"âŒ æ— æ³•å¯åŠ¨ Chrome é©±åŠ¨: {e}")
            return False
    
    def has_version_info(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ç‰ˆæœ¬ä¿¡æ¯"""
        if not text:
            return False
        return "1." in text
    
    def scrape_with_selenium(self) -> List[VersionRecord]:
        """ä½¿ç”¨ Selenium çˆ¬å–"""
        if not self.setup_driver():
            return []
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {self.history_url}")
            self.driver.get(self.history_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            print("âœ… é¡µé¢åŠ è½½å®Œæˆ")
            time.sleep(2)  # é¢å¤–ç­‰å¾…
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            print(f"ğŸ“„ é¡µé¢å¤§å°: {len(page_source)} å­—ç¬¦")
            
            # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
            with open("debug_page_source.html", "w", encoding="utf-8") as f:
                f.write(page_source)
            print("ğŸ’¾ å·²ä¿å­˜é¡µé¢æºç åˆ° debug_page_source.html")
            
            return self.parse_selenium_html(page_source)
            
        finally:
            if self.driver:
                self.driver.quit()
    
    def parse_selenium_html(self, html: str) -> List[VersionRecord]:
        """è§£æ Selenium è·å–çš„å†å²è®°å½•é¡µé¢ HTML"""
        records = []
        
        print("ğŸ” å¼€å§‹è§£æå†å²è®°å½•é¡µé¢...")
        
        print("ğŸ”„ ç›´æ¥è§£æ data-mw-revid å’ŒåŸºæœ¬ä¿¡æ¯...")
        
        # ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰åŒ…å« oldid çš„è¡Œ
        oldid_lines = re.findall(r'<li[^>]*data-mw-revid="(\d+)"[^>]*>(.*?)</li>', html, re.DOTALL)
        
        print(f"ğŸ” æ‰¾åˆ° {len(oldid_lines)} ä¸ªä¿®è®¢è®°å½•")
        matches = []  # è®¾ç½®ä¸ºç©ºï¼Œå¼ºåˆ¶ä½¿ç”¨ä¸‹é¢çš„é€»è¾‘
        
        # å¤„ç†æ‰¾åˆ°çš„ä¿®è®¢è®°å½•
        for oldid, line_content in oldid_lines:
            # æå–æ—¶é—´æˆ³
            time_match = re.search(r'(\d{1,2}:\d{2}), (\d{1,2} \w+ \d{4})', line_content)
            if not time_match:
                continue
                
            time_part, date_part = time_match.groups()
            
            # æå–ç¼–è¾‘è€…
            editor_match = re.search(r'<a[^>]*title="User:([^"]*)"[^>]*><bdi>([^<]*)</bdi></a>', line_content)
            editor = editor_match.group(2) if editor_match else "Unknown"
            
            # æå–æ–‡ä»¶å¤§å°
            size_match = re.search(r'(\d{1,3}(?:,\d{3})*)\s+bytes', line_content)
            file_size = size_match.group(1) if size_match else "Unknown"
            
            # æå–å˜æ›´
            change_match = re.search(r'([+-]\d+)', line_content)
            change_bytes = change_match.group(1) if change_match else "0"
            
            # æå–æ³¨é‡Š (å¤„ç†åµŒå¥— HTML æ ‡ç­¾)
            # æŸ¥æ‰¾ comment span çš„å¼€å§‹ä½ç½®
            comment_start = re.search(r'<span[^>]*class="comment[^"]*"[^>]*>', line_content)
            if comment_start:
                start_pos = comment_start.end()
                # ä»å¼€å§‹ä½ç½®æŸ¥æ‰¾åŒ¹é…çš„ç»“æŸæ ‡ç­¾ï¼Œè€ƒè™‘åµŒå¥—
                span_count = 1
                pos = start_pos
                while pos < len(line_content) and span_count > 0:
                    span_open = line_content.find('<span', pos)
                    span_close = line_content.find('</span>', pos)
                    
                    if span_close == -1:
                        break
                    
                    if span_open != -1 and span_open < span_close:
                        span_count += 1
                        pos = span_open + 5
                    else:
                        span_count -= 1
                        pos = span_close + 7
                        
                if span_count == 0:
                    comment_html = line_content[start_pos:pos-7]  # å‡å» </span> çš„é•¿åº¦
                    # ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
                    comment = re.sub(r'<[^>]+>', '', comment_html).strip()
                    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                    comment = re.sub(r'\s+', ' ', comment).strip()
                else:
                    comment = ""
            else:
                comment = ""
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰ˆæœ¬ä¿¡æ¯
            if self.has_version_info(comment):
                wiki_url = f"{self.base_url}/w/Java_Edition_protocol/Entity_metadata?oldid={oldid}"
                edit_url = f"{self.base_url}/w/Java_Edition_protocol/Entity_metadata?action=edit&oldid={oldid}"
                
                record = VersionRecord(
                    date=date_part,
                    time=time_part,
                    editor=editor,
                    file_size=file_size,
                    change_bytes=change_bytes,
                    description=comment,
                    wiki_url=wiki_url,
                    edit_url=edit_url
                )
                records.append(record)
                print(f"âœ… å‘ç°ç‰ˆæœ¬è®°å½•: ({date_part}) - oldid={oldid} - {comment[:50]}...")
            else:
                # è·³è¿‡æ²¡æœ‰ç‰ˆæœ¬ä¿¡æ¯çš„è®°å½•
                if comment:
                    # ç‰¹æ®Šè®°å½•ï¼šå¦‚æœæ˜¯å¯ç–‘çš„é—æ¼ï¼Œæ˜¾ç¤ºå®Œæ•´ comment
                    if "entity" in comment.lower() or "metadata" in comment.lower():
                        print(f"â­ï¸  è·³è¿‡æ— ç‰ˆæœ¬: oldid={oldid} - FULL: {comment}")
                    else:
                        print(f"â­ï¸  è·³è¿‡æ— ç‰ˆæœ¬: oldid={oldid} - {comment[:50]}...")
                else:
                    print(f"â­ï¸  è·³è¿‡æ— è¯„è®º: oldid={oldid}")
        
        print(f"ğŸ¯ è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(records)} æ¡è®°å½•")
        return records





def save_results(records: List[VersionRecord]):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if not records:
        return
    
    # JSON
    with open("minecraft_complete_versions.json", 'w', encoding='utf-8') as f:
        json.dump([asdict(r) for r in records], f, ensure_ascii=False, indent=2)
    
    
    print(f"ğŸ’¾ å·²ä¿å­˜: minecraft_complete_versions.json")


def parse_saved_page():
    """è§£æå·²ä¿å­˜çš„é¡µé¢å†…å®¹"""
    print("ğŸš€ è§£æå·²ä¿å­˜çš„é¡µé¢å†…å®¹")
    print("=" * 60)
    
    try:
        with open("debug_page_source.html", "r", encoding="utf-8") as f:
            html = f.read()
        
        print(f"ğŸ“„ è¯»å–é¡µé¢å†…å®¹: {len(html):,} å­—ç¬¦")
        
        scraper = SeleniumScraper()
        records = scraper.parse_selenium_html(html)
        
        if records:
            print(f"\nâœ… æˆåŠŸè§£æ {len(records)} ä¸ªç‰ˆæœ¬è®°å½•")
            
            # æ˜¾ç¤ºå‰10ä¸ªç‰ˆæœ¬
            print("\nğŸ“Š å‘ç°çš„ç‰ˆæœ¬è®°å½•:")
            for i, record in enumerate(records[:10]):
                oldid = record.wiki_url.split('=')[-1]
                comment_preview = record.description[:30] + "..." if len(record.description) > 30 else record.description
                print(f"  {i+1:2d}. {record.date:15} | {record.editor:12} | oldid={oldid} | {comment_preview}")
            
            if len(records) > 10:
                print(f"  ... è¿˜æœ‰ {len(records) - 10} ä¸ªç‰ˆæœ¬")
            
            save_results(records)
            print(f"\nğŸ¯ å®Œæˆï¼å…±å¤„ç† {len(records)} ä¸ª Minecraft ç‰ˆæœ¬")
        else:
            print("âŒ æœªèƒ½è§£æä»»ä½•ç‰ˆæœ¬æ•°æ®")
            
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ° debug_page_source.html æ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ Selenium çˆ¬è™«è·å–é¡µé¢å†…å®¹")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Minecraft ç‰ˆæœ¬å†å²çˆ¬è™«")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿å­˜çš„é¡µé¢å†…å®¹
    import os
    if os.path.exists("debug_page_source.html"):
        print("ğŸ“„ å‘ç°å·²ä¿å­˜çš„é¡µé¢å†…å®¹ï¼Œç›´æ¥è§£æ...")
        parse_saved_page()
        return
    
    if not SELENIUM_AVAILABLE:
        print("âŒ Selenium æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install selenium")
        print("âŒ è¿˜éœ€è¦ä¸‹è½½ ChromeDriver: https://chromedriver.chromium.org/")
        return
    
    selenium_scraper = SeleniumScraper()
    records = selenium_scraper.scrape_with_selenium()
    
    if records:
        print(f"\nâœ… æˆåŠŸè·å– {len(records)} ä¸ªç‰ˆæœ¬è®°å½•")
        
        # æ˜¾ç¤ºå‰10ä¸ªç‰ˆæœ¬
        print("\nğŸ“Š å‘ç°çš„ç‰ˆæœ¬è®°å½•:")
        for i, record in enumerate(records[:10]):
            oldid = record.wiki_url.split('=')[-1]
            comment_preview = record.description[:30] + "..." if len(record.description) > 30 else record.description
            print(f"  {i+1:2d}. {record.date:15} | {record.editor:12} | oldid={oldid} | {comment_preview}")
        
        if len(records) > 10:
            print(f"  ... è¿˜æœ‰ {len(records) - 10} ä¸ªè®°å½•")
        
        save_results(records)
        print(f"\nğŸ¯ å®Œæˆï¼å…±å¤„ç† {len(records)} ä¸ª Minecraft ç‰ˆæœ¬")
    else:
        print("âŒ æœªèƒ½è·å–ä»»ä½•ç‰ˆæœ¬æ•°æ®")


if __name__ == "__main__":
    main()
